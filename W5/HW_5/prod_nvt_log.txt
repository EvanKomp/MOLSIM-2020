                   :-) GROMACS - gmx mdrun, VERSION 5.1.4 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra   Sebastian Fritsch 
  Gerrit Groenhof   Christoph Junghans   Anca Hamuraru    Vincent Hindriksen
 Dimitrios Karkoulis    Peter Kasson        Jiri Kraus      Carsten Kutzner  
    Per Larsson      Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff 
   Erik Marklund      Teemu Murtola       Szilard Pall       Sander Pronk   
   Roland Schulz     Alexey Shvetsov     Michael Shirts     Alfons Sijbers  
   Peter Tieleman    Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, VERSION 5.1.4
Executable:   /sw/contrib/gromacs-5.1.4-with-plumed-2.4.3-with-impi/gromacs-5.1.4-with-plumed-2.4.3-withimpi//bin/gmx_mpi
Data prefix:  /sw/contrib/gromacs-5.1.4-with-plumed-2.4.3-with-impi/gromacs-5.1.4-with-plumed-2.4.3-withimpi/
Command line:
  gmx_mpi mdrun -s prod_nvt.tpr -c prod_nvt.gro -e prod_nvt.edr -o prod_nvt.trr -g prod_nvt.log -v


Back Off! I just backed up prod_nvt.log to ./#prod_nvt.log.2#

Running on 1 node with total 40 cores, 40 logical cores
Hardware detected on host n2453.hyak.local (the node of MPI rank 0):
  CPU info:
    Vendor: GenuineIntel
    Brand:  Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

Reading file prod_nvt.tpr, VERSION 5.1.4 (single precision)
Changing nstlist from 10 to 40, rlist from 1.2 to 1.2

Using 1 MPI process
Using 40 OpenMP threads 


-------------------------------------------------------
Program gmx mdrun, VERSION 5.1.4
Source code file: /sw/contrib/gromacs-5.1.4-with-plumed-2.4.3-with-impi/gromacs-5.1.4-with-plumed-2.4.3-withimpi/src/programs/mdrun/resource-division.cpp, line: 571

Fatal error:
Your choice of 1 MPI rank and the use of 40 total threads leads to the use of 40 OpenMP threads, whereas we expect the optimum to be with more MPI ranks with 1 to 6 OpenMP threads. If you want to run with this many OpenMP threads, specify the -ntomp option. But we suggest to increase the number of MPI ranks.
For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------

Halting program gmx mdrun
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 0
[unset]: write_line error; fd=-1 buf=:cmd=abort exitcode=1
:
system msg for write_line failure : Bad file descriptor
